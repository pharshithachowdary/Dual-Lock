{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf185e7",
   "metadata": {},
   "source": [
    "#  Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d68eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import operator\n",
    "import itertools\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from csv import reader\n",
    "from random import randrange\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "#FORMAT = '%(asctime)s %(levelname)s %(message)s'\n",
    "#logging.basicConfig(format=FORMAT)\n",
    "#logger = logging.getLogger('tensorflow')\n",
    "\n",
    "logger = logging.getLogger('tensorflow')\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.removeHandler(logger.handlers[0])\n",
    "logger.propagate = False\n",
    "\n",
    "def sales_example(sales):\n",
    "  record = {\n",
    "        'sales': tf.train.Feature(float_list=tf.train.FloatList(value=sales))\n",
    "  }\n",
    "\n",
    "  return tf.train.Example(features=tf.train.Features(feature=record))\n",
    "\n",
    "def capacity_example(capacity):\n",
    "  record = {\n",
    "        'capacity': tf.train.Feature(float_list=tf.train.FloatList(value=capacity))\n",
    "  }\n",
    "\n",
    "  return tf.train.Example(features=tf.train.Features(feature=record))\n",
    "\n",
    "def stock_example(stock):\n",
    "  record = {\n",
    "        'stock': tf.train.Feature(float_list=tf.train.FloatList(value=stock))\n",
    "  }\n",
    "\n",
    "  return tf.train.Example(features=tf.train.Features(feature=record))\n",
    "\n",
    "#https://stackoverflow.com/questions/553303/generate-a-random-date-between-two-other-dates\n",
    "def random_date(start, end):\n",
    "  return start + datetime.timedelta(\n",
    "    seconds=random.randint(0, int((end - start).total_seconds())),\n",
    "  )\n",
    "\n",
    "def create_records(number_of_products, start_date, end_date, start_time_period, middle_time_period, end_time_period, orders_file, products_file, departments_file, order_products_prior_file, order_products_train_file, train_tfrecords_file, test_tfrecords_file, capacity_tfrecords_file, stock_tfrecords_file):\n",
    "\n",
    "  stock = np.random.uniform(low=0.0, high=1.0, size=(FLAGS.number_of_products))\n",
    "  with tf.io.TFRecordWriter(stock_tfrecords_file) as writer:\n",
    "    logger.debug (\"stock: {}\".format(stock))\n",
    "    tf_example = stock_example(stock)\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "\n",
    "  with open(orders_file, 'r') as f:\n",
    "    csv_reader = reader(f)\n",
    "    next(csv_reader)\n",
    "    orders_list = list(map(tuple, csv_reader))\n",
    "\n",
    "  sorted_orders = sorted(orders_list, key = lambda x: (int(x[1]), int(x[3])))\n",
    "\n",
    "  dated_orders = []\n",
    "\n",
    "  i = 0\n",
    "  for k, g in itertools.groupby(sorted_orders, lambda x : int(x[1])):\n",
    "    item = next(g)\n",
    "    order_date = random_date(start_date, end_date)\n",
    "    while order_date.weekday() != int(item[4]):\n",
    "      order_date = order_date + datetime.timedelta(days=1)\n",
    "        \n",
    "    start_date = datetime.datetime.combine(start_date, datetime.datetime.min.time())\n",
    "    end_date = datetime.datetime.combine(end_date, datetime.datetime.min.time())\n",
    "\n",
    "    order_date = datetime.datetime(order_date.year, order_date.month, order_date.day, int(item[5]), 0, 0) \n",
    "    time_period = int((order_date - start_date).total_seconds() / (60*60*6))\n",
    "    dated_orders.append((int(item[0]), int(item[1]), int(item[4]), order_date, time_period))\n",
    "    \n",
    "    for item in g:\n",
    "      order_date = order_date + datetime.timedelta(days=int(float(item[6])))\n",
    "      order_date = datetime.datetime(order_date.year, order_date.month, order_date.day, int(item[5]), 0, 0)\n",
    "      time_period = int((order_date - start_date).total_seconds() / (60*60*6))\n",
    "      dated_orders.append((int(item[0]), int(item[1]), int(item[4]), order_date, time_period))\n",
    "\n",
    "  orders = pd.DataFrame(dated_orders, columns =['order_id', 'user_id', 'order_dow', 'order_date', 'time_period'])\n",
    "\n",
    "  products = pd.read_csv(\"products.csv\")\n",
    "  departments = pd.read_csv(\"departments.csv\")\n",
    "  prior_order = pd.read_csv(\"order_products__prior.csv\")\n",
    "  train_order = pd.read_csv(\"order_products__train.csv\")\n",
    "\n",
    "  #aisles = pd.read_csv(\"aisles.csv\")\n",
    "\n",
    "  ntop = int(FLAGS.top_products*products['product_id'].count())\n",
    "\n",
    "  all_ordered_products = pd.concat([prior_order, train_order], axis=0)[[\"order_id\", \"product_id\"]]\n",
    "\n",
    "  largest = all_ordered_products[['product_id']].groupby(['product_id']).size().nlargest(ntop).to_frame()\n",
    "  largest.reset_index(inplace=True)\n",
    "\n",
    "  products_largest = pd.merge(largest, products, how=\"left\", on=\"product_id\")[['product_id', 'product_name', 'aisle_id', 'department_id']]\n",
    "\n",
    "  products_departments = pd.merge(products_largest, departments, how=\"left\", on=\"department_id\")\n",
    "\n",
    "  products_departments = products_departments[products_departments[\"department\"].isin([\"frozen\", \"bakery\", \"produce\", \"beverages\", \"dry goods pasta\", \"meat seafood\", \"pantry\", \"breakfast\", \"canned goods\", \"dairy eggs\", \"snacks\", \"deli\"])]\n",
    "\n",
    "  products_departments_list = products_departments.values.tolist()\n",
    "\n",
    "  products_subset=set()\n",
    "  while len(products_subset) < number_of_products:\n",
    "    products_subset.add((random.randint(0,len(products_departments_list))))\n",
    "\n",
    "  selected_products_departments_list = [products_departments_list[i] for i in products_subset]\n",
    "  selected_products_list = [products_departments_list[i][0] for i in products_subset]\n",
    "\n",
    "  for p, product_id in enumerate(selected_products_list):\n",
    "    logger.info (\"{} {}\".format(p, product_id))\n",
    "\n",
    "  selected_products_departments = pd.DataFrame(selected_products_departments_list, columns =['product_id', 'product_name', 'aisle_id', 'department_id', 'department'])\n",
    "\n",
    "  all_ordered_products_quantity_list = []\n",
    "  for item in all_ordered_products.itertuples():\n",
    "    all_ordered_products_quantity_list.append((item[1], item[2], 1))\n",
    "    #all_ordered_products_quantity_list.append((item[1], item[2], random.randint(1, 6)))\n",
    "\n",
    "  all_ordered_products_quantity = pd.DataFrame(all_ordered_products_quantity_list, columns =[\"order_id\", \"product_id\", 'quantity'])\n",
    "\n",
    "  order_product_departments = pd.merge(selected_products_departments, all_ordered_products_quantity, how=\"left\", on=\"product_id\")\n",
    "  order_product_departments_dates = pd.merge(order_product_departments, orders, how=\"left\", on=\"order_id\")\n",
    "\n",
    "  grocery = order_product_departments_dates[[\"order_id\", \"product_id\", \"product_name\", \"order_date\", \"time_period\", 'quantity']]\n",
    "\n",
    "  shelf_capacity = ((grocery.groupby(['product_id'])['quantity'].sum()/grocery['time_period'].nunique())*4*3).to_frame()\n",
    "  shelf_capacity.reset_index(inplace=True)\n",
    "\n",
    "  with tf.io.TFRecordWriter(capacity_tfrecords_file) as writer:\n",
    "    capacity = []\n",
    "    for p, product_id in enumerate(selected_products_list):\n",
    "      capacity.append(math.ceil(shelf_capacity[shelf_capacity['product_id'] == product_id]['quantity'].values[0]))\n",
    "\n",
    "    logger.debug (\"capacity: {}\".format(capacity))\n",
    "    tf_example = capacity_example(np.array(capacity, dtype=np.float32))\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "\n",
    "  counter = 0\n",
    "  with tf.io.TFRecordWriter(train_tfrecords_file) as writer:\n",
    "    for t in range(start_time_period, middle_time_period):\n",
    "      sales = []\n",
    "      for p, product_id in enumerate(selected_products_list):\n",
    "        sales.append(grocery[(grocery['time_period'] == t) & (grocery['product_id'] == product_id)]['quantity'].sum())\n",
    " \n",
    "      logger.debug (\"pediod {}: {}\".format(t, sales))\n",
    "      tf_example = sales_example(np.array(sales, dtype=np.float32))\n",
    "      writer.write(tf_example.SerializeToString())\n",
    "      counter = counter + 1\n",
    "\n",
    "  logger.info (\"created {} train sales records\".format(counter))\n",
    "\n",
    "  if end_time_period == -1:\n",
    "    end_time_period = grocery['time_period'].max()\n",
    "\n",
    "  counter = 0\n",
    "  with tf.io.TFRecordWriter(test_tfrecords_file) as writer:\n",
    "    for t in range(middle_time_period, end_time_period+1):\n",
    "      sales = []\n",
    "      for p, product_id in enumerate(selected_products_list):\n",
    "        sales.append(grocery[(grocery['time_period'] == t) & (grocery['product_id'] == product_id)]['quantity'].sum())\n",
    " \n",
    "      logger.debug (\"pediod {}: {}\".format(t, sales))\n",
    "      tf_example = sales_example(np.array(sales, dtype=np.float32))\n",
    "      writer.write(tf_example.SerializeToString())\n",
    "      counter = counter + 1\n",
    "\n",
    "  logger.info (\"created {} test sales records\".format(counter))\n",
    "\n",
    "def main():\n",
    "  create_records(FLAGS.number_of_products, FLAGS.start_date, FLAGS.end_date, FLAGS.start_time_period, FLAGS.middle_time_period, FLAGS.end_time_period, FLAGS.orders_file, FLAGS.products_file, FLAGS.departments_file, FLAGS.order_products_prior_file, FLAGS.order_products_train_file, FLAGS.train_tfrecords_file, FLAGS.test_tfrecords_file, FLAGS.capacity_tfrecords_file, FLAGS.stock_tfrecords_file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "\n",
    "  parser.add_argument('--number_of_products', type=int, default=100,\n",
    "            help='Subset of products from whole ~50k dataset.')\n",
    "  parser.add_argument('--top_products', type=float, default=0.2,\n",
    "            help='Top percentage of products to consider, so shelf capacity equal to 3-days of sales will have a reasonable number > 1.')\n",
    "  parser.add_argument('--start_date', type=datetime.date.fromisoformat, default='2017-01-01',\n",
    "            help='Start date random range to create timestampts.')\n",
    "  parser.add_argument('--end_date', type=datetime.date.fromisoformat, default='2017-01-06',\n",
    "            help='End date random range to create timestampts.')\n",
    "  parser.add_argument('--start_time_period', type=int, default=0,\n",
    "            help='Start timestep for train dataset.')\n",
    "  parser.add_argument('--middle_time_period', type=int, default=1000,\n",
    "            help='End timestep for train dataset and this is the first timestep for test dataset.')\n",
    "  parser.add_argument('--end_time_period', type=int, default=-1,\n",
    "            help='Last timestep for test dataset. If -1 than until the end of data.')\n",
    "  parser.add_argument('--orders_file', type=str, default='orders.csv',\n",
    "            help='orders file location.')\n",
    "  parser.add_argument('--products_file', type=str, default='products.csv',\n",
    "            help='products file location.')\n",
    "  parser.add_argument('--departments_file', type=str, default='departments.csv',\n",
    "            help='departments file location.')\n",
    "  parser.add_argument('--order_products_prior_file', type=str, default='order_products__prior.csv',\n",
    "            help='order_products_prior file location.')\n",
    "  parser.add_argument('--order_products_train_file', type=str, default='order_products__train.csv',\n",
    "            help='order_products_train file location.')\n",
    "  parser.add_argument('--logging', default='INFO', choices=['DEBUG','INFO','WARNING','ERROR','CRITICAL'],\n",
    "            help='Enable excessive variables screen outputs.')\n",
    "  parser.add_argument('--train_tfrecords_file', type=str, default='train.tfrecords',\n",
    "            help='train sales tfrecords output file')\n",
    "  parser.add_argument('--test_tfrecords_file', type=str, default='test.tfrecords',\n",
    "            help='test sales tfrecords output file')\n",
    "  parser.add_argument('--capacity_tfrecords_file', type=str, default='capacity.tfrecords',\n",
    "            help='shelf capacity tfrecords output file, train or test')\n",
    "  parser.add_argument('--stock_tfrecords_file', type=str, default='stock.tfrecords',\n",
    "            help='Stock data for each product for predict mode.')\n",
    "\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "  logger.setLevel(FLAGS.logging)\n",
    "\n",
    "  logger.debug (\"Running with parameters: {}\".format(FLAGS))\n",
    "\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab33bed",
   "metadata": {},
   "source": [
    "# Traning the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27987ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=25, linewidth=10000, precision=12, suppress=True)\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "class Dense(tf.Module):\n",
    "  def __init__(self, input_dim, output_size, activation=None, stddev=1.0):\n",
    "    super(Dense, self).__init__()\n",
    "    self.w = tf.Variable(\n",
    "      tf.random.truncated_normal([input_dim, output_size], stddev=stddev), name='w')\n",
    "    self.b = tf.Variable(tf.zeros([output_size]), name='b')\n",
    "    self.activation = activation\n",
    "  def __call__(self, x):\n",
    "    y = tf.matmul(x, self.w) + self.b\n",
    "    if (self.activation):\n",
    "      y = self.activation(y)\n",
    "    return y\n",
    "\n",
    "#Policy network\n",
    "class Actor(tf.Module):\n",
    "  def __init__(self, num_features, num_actions, hidden_size, activation=tf.nn.relu, dropout_prob=0.1):\n",
    "    super(Actor, self).__init__()\n",
    "    self.layer1 = Dense(num_features, hidden_size, activation=None)\n",
    "    self.layer2 = Dense(hidden_size, hidden_size, activation=None)\n",
    "    self.layer3 = Dense(hidden_size, hidden_size, activation=None)\n",
    "    self.layer4 = Dense(hidden_size, num_actions, activation=None)\n",
    "    self.activation = activation\n",
    "    self.dropout_prob = dropout_prob\n",
    "  def __call__(self, state):\n",
    "    #[I, P] --> [I]\n",
    "    layer_output = self.layer1(state)\n",
    "    #layer_output = tfa.layers.GroupNormalization(groups = 1)(layer_output) \n",
    "    layer_output = self.activation(layer_output)\n",
    "    layer_output = tf.nn.dropout(layer_output, self.dropout_prob)\n",
    "\n",
    "    layer_output = self.layer2(layer_output)\n",
    "    #layer_output = tfa.layers.GroupNormalization(groups = 1)(layer_output) \n",
    "    layer_output = self.activation(layer_output)\n",
    "    layer_output = tf.nn.dropout(layer_output, self.dropout_prob)\n",
    "\n",
    "    layer_output = self.layer3(layer_output)\n",
    "    #layer_output = tfa.layers.GroupNormalization(groups = 1)(layer_output) \n",
    "    layer_output = self.activation(layer_output)\n",
    "    layer_output = tf.nn.dropout(layer_output, self.dropout_prob)\n",
    "\n",
    "    layer_output = self.layer4(layer_output)\n",
    "    #tf.print(\"layer_output:\", tf.reduce_mean(layer_output, keepdims=False), layer_output[:10], output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "    # 0 <= u <= 1 eq 3\n",
    "    #layer_output = tf.nn.sigmoid(layer_output)\n",
    "    return tf.nn.softmax(layer_output)\n",
    "    #tf.print(\"sigmoid:\", tf.reduce_mean(layer_output, keepdims=False), layer_output[:10], output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "    #[I, 1] --> [I]\n",
    "    #return tf.squeeze(layer_output, axis=-1, name='factor_squeeze')\n",
    "\n",
    "#Value network\n",
    "class Critic(tf.Module):\n",
    "  def __init__(self, num_features, hidden_size, activation=tf.nn.relu, dropout_prob=0.1):\n",
    "    super(Critic, self).__init__()\n",
    "    self.layer1 = Dense(num_features, hidden_size, activation=None)\n",
    "    self.layer2 = Dense(hidden_size, 1, activation=None)\n",
    "    self.activation = activation\n",
    "    self.dropout_prob = dropout_prob\n",
    "  def __call__(self, state):\n",
    "    #[I, P] --> [I]\n",
    "    layer_output = self.layer1(state)\n",
    "    layer_output = tfa.layers.GroupNormalization(groups = 1)(layer_output) \n",
    "    layer_output = self.activation(layer_output)\n",
    "    layer_output = tf.nn.dropout(layer_output, self.dropout_prob)\n",
    "\n",
    "    layer_output = self.layer2(layer_output)\n",
    "\n",
    "    #[I, 1] --> [I]\n",
    "    return tf.squeeze(layer_output, axis=-1, name='factor_squeeze')\n",
    "\n",
    "def sales_parser(serialized_example):\n",
    "  example = tf.io.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "      \"sales\": tf.io.FixedLenFeature([FLAGS.num_products], tf.float32)\n",
    "    })\n",
    "\n",
    "  for name in list(example.keys()):\n",
    "    t = example[name]\n",
    "    if t.dtype == tf.int64:\n",
    "      t = tf.to_float32(t)\n",
    "      example[name] = t\n",
    "\n",
    "  return example\n",
    "\n",
    "def capacity_parser(serialized_example):\n",
    "  example = tf.io.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "      \"capacity\": tf.io.FixedLenFeature([FLAGS.num_products], tf.float32)\n",
    "    })\n",
    "\n",
    "  for name in list(example.keys()):\n",
    "    t = example[name]\n",
    "    if t.dtype == tf.int64:\n",
    "      t = tf.to_float32(t)\n",
    "      example[name] = t\n",
    "\n",
    "  return example\n",
    "\n",
    "def stock_parser(serialized_example):\n",
    "  example = tf.io.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "      \"stock\": tf.io.FixedLenFeature([FLAGS.num_products], tf.float32)\n",
    "    })\n",
    "\n",
    "  for name in list(example.keys()):\n",
    "    t = example[name]\n",
    "    if t.dtype == tf.int64:\n",
    "      t = tf.to_float32(t)\n",
    "      example[name] = t\n",
    "\n",
    "  return example\n",
    "\n",
    "def waste(x):\n",
    "   return FLAGS.waste * x\n",
    "\n",
    "def quantile(x, q):\n",
    "\n",
    "  return np.quantile(x, q)\n",
    "\n",
    "  x = tf.sort(x, direction='ASCENDING')\n",
    "  pos = q * tf.cast(tf.size(x), tf.float32)\n",
    "  floor_pos = tf.floor(pos)\n",
    "  int_pos = tf.cast(floor_pos, tf.int32)\n",
    "   \n",
    "  v_diff = x[int_pos+1]-x[int_pos]\n",
    "  p_diff = pos - floor_pos\n",
    "   \n",
    "  return x[int_pos]+v_diff*p_diff\n",
    "\n",
    "def cross_entropy(p, q):\n",
    "  return -tf.reduce_mean(tf.reduce_sum(p*tf.math.log(tf.math.maximum(1e-15, q)), axis=1))\n",
    "\n",
    "class Env(tf.Module):\n",
    "  def __init__(self, num_features, hidden_size, activation=tf.nn.relu, dropout_prob=0.1):\n",
    "    super(Critic, self).__init__()\n",
    "    self.layer1 = Dense(num_features, hidden_size, activation=None)\n",
    "    self.layer2 = Dense(hidden_size, 1, activation=None)\n",
    "    self.activation = activation\n",
    "    self.dropout_prob = dropout_prob\n",
    "  def __call__(self, u):\n",
    "    #[I, P] --> [I]\n",
    "    layer_output = self.layer1(state)\n",
    "    layer_output = tfa.layers.GroupNormalization(groups = 1)(layer_output) \n",
    "    layer_output = self.activation(layer_output)\n",
    "    layer_output = tf.nn.dropout(layer_output, self.dropout_prob)\n",
    "\n",
    "    layer_output = self.layer2(layer_output)\n",
    "\n",
    "    #[I, 1] --> [I]\n",
    "    return tf.squeeze(layer_output, axis=-1, name='factor_squeeze')\n",
    "\n",
    "def predict():\n",
    "  sales_dataset = tf.data.TFRecordDataset(FLAGS.predict_file)\n",
    "  capacity_dataset = tf.data.TFRecordDataset(FLAGS.capacity_file)\n",
    "  stock_dataset = tf.data.TFRecordDataset(FLAGS.stock_file)\n",
    "\n",
    "  parsed_capacity_dataset = capacity_dataset.map(capacity_parser)\n",
    "  capacity = next(iter(parsed_capacity_dataset))['capacity']\n",
    "\n",
    "  parsed_dataset = sales_dataset.map(sales_parser)\n",
    "\n",
    "  parsed_stock_dataset = stock_dataset.map(stock_parser)\n",
    "  x = next(iter(parsed_stock_dataset))['stock']\n",
    "  #x = tf.divide(next(iter(parsed_stock_dataset))['stock'], capacity)\n",
    "\n",
    "  actor = Actor(FLAGS.num_features, FLAGS.num_actions, FLAGS.hidden_size, activation=tf.nn.relu, dropout_prob=FLAGS.dropout_prob)\n",
    "\n",
    "  checkpoint = tf.train.Checkpoint(actor=actor)\n",
    "  checkpoint.restore(tf.train.latest_checkpoint(FLAGS.output_dir)).expect_partial()\n",
    "\n",
    "  with tf.io.gfile.GFile(FLAGS.output_file, \"w\") as writer:\n",
    "    for sales_record in parsed_dataset:\n",
    "      \n",
    "      sales = tf.divide(sales_record['sales'], capacity)\n",
    "\n",
    "      q = waste(x)\n",
    "\n",
    "      s = tf.transpose(tf.stack([x, sales, q], axis=0), perm=[1, 0])\n",
    "\n",
    "      policy_probs = actor(s)\n",
    "      policy_mask = tf.one_hot(tf.math.argmax(policy_probs, axis=-1), FLAGS.num_actions)\n",
    "      action_space = tf.tile([[0, 0.005, 0.01, 0.0125, 0.015, 0.0175, 0.02, 0.03, 0.04, 0.08, 0.12, 0.2, 0.5, 1]], [FLAGS.num_products, 1])\n",
    "      u = tf.boolean_mask(action_space, policy_mask)\n",
    "\n",
    "      overstock = tf.math.maximum(0, (x + u) - 1)\n",
    "\n",
    "      x_u = tf.math.minimum(1, x + u)\n",
    "\n",
    "      stockout = tf.math.minimum(0, x_u - sales)\n",
    "\n",
    "      writer.write(\"stock:\" + ','.join(  list(map(str,   x.numpy()    ))    ) + \"\\n\")\n",
    "      writer.write(\"action:\" + ','.join(  list(map(str,   u.numpy()    ))    ) + \"\\n\")\n",
    "      writer.write(\"overstock:\" + ','.join(  list(map(str,   overstock.numpy()    ))    ) + \"\\n\")\n",
    "      writer.write(\"sales:\" + ','.join(  list(map(str,   sales.numpy()    ))    ) + \"\\n\")\n",
    "      writer.write(\"stockout:\" + ','.join(  list(map(str,   stockout.numpy()    ))    ) + \"\\n\")\n",
    "      writer.write(\"capacity:\" + ','.join(  list(map(str,   (capacity/capacity).numpy()    ))    ) + \"\\n\")\n",
    "\n",
    "      x = tf.math.maximum(0, x_u - sales)\n",
    "\n",
    "def train():\n",
    "  #   LEGEND:\n",
    "  #   p - number of products\n",
    "  #   f - number of features\n",
    "  #   t - number of timesteps in an episode\n",
    "  #   n - number of actions\n",
    "  #   ep - experience collection episodes\n",
    "\n",
    "  #sales for FLAGS.num_timesteps time periods for NUM_PRODUCTS products. Sales for period [t, t+1]. so index t=0, sales from 0 until 1 \n",
    "  sales_dataset = tf.data.TFRecordDataset(FLAGS.train_file).window(FLAGS.batch_size, shift=FLAGS.batch_size-1, drop_remainder=False)\n",
    "\n",
    "  capacity_dataset = tf.data.TFRecordDataset(FLAGS.capacity_file) #, buffer_size=FLAGS.dataset_reader_buffer_size)\n",
    "  parsed_capacity_dataset = capacity_dataset.map(capacity_parser)\n",
    "  capacity = next(iter(parsed_capacity_dataset))['capacity']\n",
    "\n",
    "  actor_optimizer = tf.optimizers.Adam(FLAGS.actor_learning_rate)\n",
    "  critic_optimizer = tf.optimizers.Adam(FLAGS.critic_learning_rate)\n",
    "\n",
    "  #Policy and Value networks with random weights \n",
    "  actor = Actor(FLAGS.num_features, FLAGS.num_actions, FLAGS.hidden_size, activation=tf.nn.relu, dropout_prob=FLAGS.dropout_prob)\n",
    "  critic = Critic(FLAGS.num_features, FLAGS.hidden_size, activation=tf.nn.relu, dropout_prob=FLAGS.dropout_prob)\n",
    "\n",
    "  #Counter\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  checkpoint_prefix = os.path.join(FLAGS.output_dir, \"ckpt\")\n",
    "  checkpoint = tf.train.Checkpoint(critic_optimizer=critic_optimizer, actor_optimizer=actor_optimizer, critic=critic, actor=actor, step=global_step)\n",
    "  status = checkpoint.restore(tf.train.latest_checkpoint(FLAGS.output_dir))\n",
    "\n",
    "  #standard deviation\n",
    "  sigma = tf.constant(0.1)\n",
    "\n",
    "  for episode in range(FLAGS.train_episodes):\n",
    "    #random initial inventory\n",
    "    # 0 <= x <= 1: eq 2\n",
    "    x = tf.random.uniform(shape=[FLAGS.num_products], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
    "    #waste 10% of grocery inventory at the begining of the day. This is q-hat estimate!\n",
    "    # q-hat: estimate of waste\n",
    "    q = waste(x)\n",
    "\n",
    "    #tf.print (\"start:\", x, output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "    for batch_dataset in sales_dataset:\n",
    "      with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "        experience_step = tf.constant(0)\n",
    "        experience_s = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products, FLAGS.num_features]), name=\"experience_s\")\n",
    "        experience_u = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products]), name=\"experience_u\")\n",
    "        experience_p = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products, FLAGS.num_actions]), name=\"experience_p\")\n",
    "        experience_i = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.int64, element_shape=tf.TensorShape([FLAGS.num_products]), name=\"experience_i\")\n",
    "        experience_pu = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products]), name=\"experience_pu\")\n",
    "        experience_overstock = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products]), name=\"experience_overstock\")\n",
    "        experience_s_prime = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products, FLAGS.num_features]), name=\"experience_s_prime\")\n",
    "        experience_r = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products]), name=\"experience_r_prime\")\n",
    "        experience_z = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products]), name=\"experience_z\")\n",
    "        experience_q = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products]), name=\"experience_q\")\n",
    "        experience_quan = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products]), name=\"experience_quan\")\n",
    "\n",
    "        batch_iterator = batch_dataset.map(sales_parser)\n",
    "\n",
    "        sales = tf.divide(next(iter(batch_iterator))['sales'], capacity)\n",
    "\n",
    "        #state is starting inventory and forecast sales during this period\n",
    "        #(p), (p) --> (f, p) --> (p, f)\n",
    "        s = tf.transpose(tf.stack([x, sales, q], axis=0), perm=[1, 0])\n",
    "\n",
    "        #tf.print(\"x:\", x, output_stream=sys.stderr, summarize=-1)\n",
    "        #tf.print(\"sales:\", sales, output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "        #(p, f) --> (p, n)\n",
    "        policy_probs = actor(s)\n",
    "\n",
    "        for item in batch_iterator:\n",
    "          sales_prime = tf.divide(item['sales'], capacity)\n",
    "\n",
    "          #(p, n) --> (p)\n",
    "          policy_index = tf.squeeze(tf.random.categorical(tf.math.log(policy_probs), 1))\n",
    "          \n",
    "          #(p) --> (p, n)\n",
    "          policy_mask = tf.one_hot(policy_index, FLAGS.num_actions)\n",
    "\n",
    "          #(p, n), (p, n) --> (p)\n",
    "          policy_selected = tf.boolean_mask(policy_probs, policy_mask)\n",
    "     \n",
    "          #tf.print(\"mask:\", mask, output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "          action_space = tf.tile([[0, 0.005, 0.01, 0.0125, 0.015, 0.0175, 0.02, 0.03, 0.04, 0.08, 0.12, 0.2, 0.5, 1]], [FLAGS.num_products, 1])\n",
    "\n",
    "          #(p, n), (p, n) --> (p)\n",
    "          u = tf.boolean_mask(action_space, policy_mask)\n",
    "\n",
    "          #tf.print(\"action:\", u, output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "          overstock = tf.math.maximum(0, (x + u) - 1)\n",
    "\n",
    "          # 0 <= x + u <= 1: eq 4 \n",
    "          #(p) + (p) --> (p)\n",
    "          x_u = tf.math.minimum(1, x + u)\n",
    "\n",
    "          # 0 <= x <= 1: eq 7\n",
    "          #(p) - (p) --> (p)\n",
    "          x_prime = tf.math.maximum(0, x_u - sales)\n",
    "\n",
    "          #tf.print(\"x_prime:\", x_prime, output_stream=sys.stderr, summarize=-1)\n",
    "        \n",
    "          #waste 10% of grocery inventory at the begining of the day. This is q-hat estimate!\n",
    "          # q-hat: estimate of waste\n",
    "          q_prime = waste(x_prime)\n",
    "\n",
    "          #(p), (p) --> (f, p) --> (p, f)\n",
    "          s_prime = tf.transpose(tf.stack([x_prime, sales_prime, q_prime], axis=0), perm=[1, 0])\n",
    "\n",
    "          z = tf.cast(x < FLAGS.zero_inventory, tf.float32)\n",
    "\n",
    "          quan = tf.repeat(tf.cast(quantile(x, 0.95) - quantile(x, 0.05), tf.float32), FLAGS.num_products)\n",
    "\n",
    "          #(p), (p), (p), (p) --> (p)\n",
    "          r = tf.cast(1 - z - overstock - q - quan, tf.float32)\n",
    "\n",
    "          #tf.print(\"rewards:\", global_step, tf.reduce_mean(r, keepdims=False), output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "          experience_s = experience_s.write(experience_step, s)\n",
    "          experience_u = experience_u.write(experience_step, u)\n",
    "          experience_p = experience_p.write(experience_step, policy_probs)\n",
    "          experience_i = experience_i.write(experience_step, policy_index)\n",
    "          experience_pu = experience_pu.write(experience_step, policy_selected)\n",
    "          experience_overstock = experience_overstock.write(experience_step, overstock)\n",
    "          experience_s_prime = experience_s_prime.write(experience_step, s_prime)\n",
    "          experience_r = experience_r.write(experience_step, r)\n",
    "          experience_z = experience_z.write(experience_step, z)\n",
    "          experience_q = experience_q.write(experience_step, q)\n",
    "          experience_quan = experience_quan.write(experience_step, quan)\n",
    "\n",
    "          #(p, f) --> (p, n)\n",
    "          policy_probs = actor(s_prime)\n",
    "\n",
    "          x = x_prime\n",
    "          q = q_prime\n",
    "          s = s_prime\n",
    "          sales = sales_prime\n",
    "\n",
    "          experience_step = experience_step + 1\n",
    "\n",
    "        #(t, p, f) --> (t*p, f)\n",
    "        s_batch = tf.reshape(experience_s.stack()[:experience_step, :, :], [-1, FLAGS.num_features])\n",
    "        x_batch = tf.reshape(experience_s.stack()[:experience_step, :, 0], [-1])\n",
    "        sal_bat = tf.reshape(experience_s.stack()[:experience_step, :, 1], [-1])\n",
    "        u_batch = tf.reshape(experience_u.stack()[:experience_step, :], [-1])\n",
    "        p_batch = tf.reshape(experience_p.stack()[:experience_step, :], [-1, FLAGS.num_actions])\n",
    "        i_batch = tf.reshape(experience_i.stack()[:experience_step, :], [-1])\n",
    "        pu_batch = tf.reshape(experience_pu.stack()[:experience_step, :], [-1])\n",
    "        overstock_batch = tf.reshape(experience_overstock.stack()[:experience_step, :], [-1])\n",
    "        s_prime_batch = tf.reshape(experience_s_prime.stack()[:experience_step, :, :], [-1, FLAGS.num_features])\n",
    "        r_batch = tf.reshape(experience_r.stack()[:experience_step, :], [-1])\n",
    "        z_batch = tf.reshape(experience_z.stack()[:experience_step, :], [-1])\n",
    "        q_batch = tf.reshape(experience_q.stack()[:experience_step, :], [-1])\n",
    "        quan_batch = tf.reshape(experience_quan.stack()[:experience_step, :], [-1])\n",
    "\n",
    "        tf.print(\"rewards:\", global_step, experience_step, tf.reduce_mean(r_batch, keepdims=False), output_stream=sys.stderr, summarize=-1)\n",
    "        tf.print(\"stockouts:\", global_step, experience_step, tf.reduce_mean(z_batch, keepdims=False), output_stream=sys.stderr, summarize=-1)\n",
    "        tf.print(\"waste:\", global_step, experience_step, tf.reduce_mean(q_batch, keepdims=False), output_stream=sys.stderr, summarize=-1)\n",
    "        tf.print(\"quantile:\", global_step, experience_step, tf.reduce_mean(quan_batch, keepdims=False), output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "        tf.print(\"x    :\", global_step, experience_step, tf.reduce_mean(x_batch, keepdims=False), output_stream=sys.stderr, summarize=-1)\n",
    "        tf.print(\"u    :\", global_step, experience_step, tf.reduce_mean(u_batch, keepdims=False), output_stream=sys.stderr, summarize=-1)\n",
    "        tf.print(\"p    :\", global_step, experience_step, tf.reduce_mean(p_batch, keepdims=False), output_stream=sys.stderr, summarize=-1)\n",
    "        tf.print(\"pu   :\", global_step, experience_step, tf.reduce_mean(pu_batch, keepdims=False), output_stream=sys.stderr, summarize=-1)\n",
    "        tf.print(\"o    :\", global_step, experience_step, tf.reduce_mean(overstock_batch, keepdims=False), output_stream=sys.stderr, summarize=-1)\n",
    "        tf.print(\"sales:\", global_step, experience_step, tf.reduce_mean(sal_bat, keepdims=False), output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "        #tf.print(\"z_batch:\", experience_step, z_batch, output_stream=sys.stderr, summarize=-1)\n",
    "        #tf.print(\"q_batch:\", experience_step, q_batch, output_stream=sys.stderr, summarize=-1)\n",
    "        #tf.print(\"quan_batch:\", experience_step, quan_batch, output_stream=sys.stderr, summarize=-1)\n",
    "        #tf.print(\"sales_batch:\", experience_step, sal_bat, output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "        #(t*p, f) --> (t*p)\n",
    "        v = critic(s_batch)\n",
    "\n",
    "        #(t*p, f) --> (t*p)\n",
    "        v_prime = critic(s_prime_batch)\n",
    "\n",
    "        y = r_batch + FLAGS.gamma*v_prime\n",
    "\n",
    "        #(t*p, t*p, t*p) --> (t*p)\n",
    "        delta = y - v\n",
    "        tf.print(\"delta:\", global_step, tf.reduce_mean(delta, keepdims=False), output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "        #(t*p) --> (1)\n",
    "        critic_loss = 0.5*tf.reduce_mean(tf.math.square(delta), keepdims=False)\n",
    "        tf.print(\"critic loss:\", global_step, critic_loss, output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "        if global_step == 0:\n",
    "          tf.print(\"p_old == p_batch:\", output_stream=sys.stderr, summarize=-1)\n",
    "          pu_old = pu_batch\n",
    "\n",
    "        #(t*p, n), (t*p, n) --> (t*p) --> (1)\n",
    "        entropy_p = cross_entropy(p_batch, p_batch)\n",
    "        tf.print(\"entropy adjusted:\", global_step, FLAGS.entropy_coefficient*entropy_p, output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "        if FLAGS.algorithm == 'A2C':\n",
    "          #(t*p), (t*p), (1) --> (1), (1) --> (1)\n",
    "          actor_loss = -tf.reduce_mean(tf.math.log(tf.math.maximum(1e-15, pu_batch))*tf.stop_gradient(delta), keepdims=False) - FLAGS.entropy_coefficient*entropy_p\n",
    "\n",
    "          #(t*p) --> (t,p) --> (p) --> (1)\n",
    "          #actor_loss = -tf.reduce_mean(tf.reduce_mean(tf.reshape(tf.math.log(tf.math.maximum(1e-15, pu_batch))*delta, [-1, FLAGS.num_products]), axis=0)) - FLAGS.entropy_coefficient*entropy_p\n",
    "        elif FLAGS.algorithm == 'A2C_mod':\n",
    "          #(t*p), ... --> (t*p,n)\n",
    "          ix_batch = tf.tile(tf.reshape(i_batch, [-1, 1]), [1, FLAGS.num_actions])\n",
    "\n",
    "          #(t*p,n) --> (t*p,n)\n",
    "          p_new = tf.nn.softmax(tf.math.log(tf.math.maximum(1e-15, p_batch)) + tf.reshape(delta, [-1, 1]) / tf.cast(tf.math.abs(ix_batch - tf.cast(tf.range(FLAGS.num_actions), tf.int64)) + 1, tf.float32))\n",
    "          #(t*p,n), ... --> (t*p)\n",
    "          #per_timestep_actor_loss = tf.reduce_mean(tf.math.squared_difference(p_batch, p_new), axis=-1)\n",
    "          #(t*p), ... --> (1)\n",
    "          actor_loss = tf.reduce_mean(per_timestep_actor_loss, axis=-1)\n",
    "        elif FLAGS.algorithm == 'PPO':\n",
    "          r = pu_batch/pu_old\n",
    "\n",
    "          #(t*p,), (t*p) --> (1)\n",
    "          actor_loss = -tf.reduce_mean(tf.math.minimum(r*delta,tf.clip_by_value(r,1-0.2,1+0.2)*delta), keepdims=False) - FLAGS.entropy_coefficient*entropy_p\n",
    "\n",
    "        tf.print(\"actor loss:\", global_step, actor_loss, output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "        pu_old = pu_batch\n",
    "\n",
    "        global_step.assign_add(1)\n",
    "\n",
    "      actor_gradients = actor_tape.gradient(actor_loss, actor.variables)\n",
    "      #tf.print(\"actor grads:\", global_step, actor_gradients, output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "      critic_gradients = critic_tape.gradient(critic_loss, critic.variables)\n",
    "      #tf.print(\"critic grads:\", global_step, critic_gradients, output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "      actor_optimizer.apply_gradients(zip(actor_gradients, actor.variables))\n",
    "      critic_optimizer.apply_gradients(zip(critic_gradients, critic.variables))\n",
    "\n",
    "    if (episode + 1) % 10 == 0:\n",
    "      checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "  ##tf.print (\"ending:\", x, output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "  tf.print (\"episode:\", episode, global_step, output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "def main():  \n",
    "  if FLAGS.action == 'TRAIN':\n",
    "    train()\n",
    "  elif FLAGS.action == 'PREDICT':\n",
    "    predict()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--output_dir', type=str, default='checkpoints',\n",
    "            help='Model directrory in google storage.')\n",
    "    parser.add_argument('--train_file', type=str, default='data/train.tfrecords',\n",
    "            help='Train file location in google storage.')\n",
    "    parser.add_argument('--capacity_file', type=str, default='data/capacity.tfrecords',\n",
    "            help='Shelf capacity file location in google storage.')\n",
    "    parser.add_argument('--stock_file', type=str, default='data/stock.tfrecords',\n",
    "            help='Stock values in prediction mode. It is random during the training.')\n",
    "    parser.add_argument('--predict_file', type=str, default='data/test.tfrecords',\n",
    "            help='Predict/Test file location in google storage.')\n",
    "    parser.add_argument('--output_file', type=str, default='./output.csv',\n",
    "            help='Prediction output.')\n",
    "    parser.add_argument('--dropout_prob', type=float, default=0.1,\n",
    "            help='This used for all dropouts.')\n",
    "    parser.add_argument('--train_episodes', type=int, default=1000,\n",
    "            help='How many times to run scenarious.')\n",
    "    parser.add_argument('--num_products', type=int, default=100,\n",
    "            help='How many productse. This is a subset of all products. They are some of grocery products.')\n",
    "    parser.add_argument('--num_timesteps', type=int, default=1000,\n",
    "            help='How many timesteps in an episode.')\n",
    "    parser.add_argument('--num_features', type=int, default=3,\n",
    "            help='How many features in Critic/Actor network.')\n",
    "    parser.add_argument('--num_actions', type=int, default=14,\n",
    "            help='How many actions for store replenishment.')\n",
    "    parser.add_argument('--hidden_size', type=int, default=32,\n",
    "            help='Actor and Critic layers hidden size.')\n",
    "    parser.add_argument('--entropy_coefficient', type=float, default=0.001,\n",
    "            help='Applied to entropy regularizing value for actor loss.')\n",
    "    parser.add_argument('--gamma', type=float, default=0.99,\n",
    "            help='Discount in future rewards.')\n",
    "    parser.add_argument('--algorithm', default='A2C', choices=['A2C','A2C_mod','PPO'],\n",
    "            help='Learning algorithm for critic and actor.')\n",
    "    parser.add_argument('--waste', type=float, default=0.025,\n",
    "            help='Waste of store stock for time period.')\n",
    "    parser.add_argument('--num_experience_episodes', type=int, default=5,\n",
    "            help='How many episodes to collect experience before starting training.')\n",
    "    parser.add_argument('--num_training_epochs', type=int, default=40,\n",
    "            help='How many epochs to train from experience buffer.')\n",
    "    parser.add_argument('--actor_learning_rate', type=float, default=0.001,\n",
    "            help='Optimizer learning rate for Actor.')\n",
    "    parser.add_argument('--critic_learning_rate', type=float, default=0.001,\n",
    "            help='Optimizer learning rate for Critic.')\n",
    "    parser.add_argument('--logging', default='INFO', choices=['DEBUG','INFO','WARNING','ERROR','CRITICAL'],\n",
    "            help='Enable excessive variables screen outputs.')\n",
    "    parser.add_argument('--zero_inventory', type=float, default=1e-5,\n",
    "            help='Consider as zero inventory if less than that.')\n",
    "    parser.add_argument('--batch_size', type=int, default=32,\n",
    "            help='Batch size.')\n",
    "    parser.add_argument('--action', default='PREDICT', choices=['TRAIN','EVALUATE','PREDICT'],\n",
    "            help='An action to execure.')\n",
    "\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cec7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef185c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
